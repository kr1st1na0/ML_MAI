{"cells":[{"cell_type":"markdown","metadata":{"id":"u_RpqYQEb6I2"},"source":["# Деревья решений"]},{"cell_type":"markdown","metadata":{"id":"fO6xDg72cZKk"},"source":["На этом семинаре попробуем сами написать дерево решений для решения задачи классификации на популярном датасете Iris"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiFbZL6gcY0E"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.datasets import load_iris"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh7T9ZYWcp9E","executionInfo":{"status":"ok","timestamp":1682197228534,"user_tz":-180,"elapsed":255,"user":{"displayName":"Kristina Bylkova","userId":"16416973404630871143"}},"outputId":"19f3b1c2-44fa-40eb-a476-48cbdcb3bef1","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["150\n"]}],"source":["iris_dataset = load_iris()\n","\n","df = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n","df['target'] = iris_dataset.target\n","df['target'] = df.target.apply(lambda v: iris_dataset.target_names[v])\n","\n","print(len(df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4TOdqGBcx8k","outputId":"d4451f7a-1435-4f65-c2fb-e638c7fa11f9"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal length (cm)</th>\n","      <th>sepal width (cm)</th>\n","      <th>petal length (cm)</th>\n","      <th>petal width (cm)</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)   \n","0                5.1               3.5                1.4               0.2  \\\n","1                4.9               3.0                1.4               0.2   \n","2                4.7               3.2                1.3               0.2   \n","3                4.6               3.1                1.5               0.2   \n","4                5.0               3.6                1.4               0.2   \n","\n","   target  \n","0  setosa  \n","1  setosa  \n","2  setosa  \n","3  setosa  \n","4  setosa  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"CLdnJKRqb90k"},"source":["## **Сделай сам**: решающий пень"]},{"cell_type":"markdown","metadata":{"id":"DddqtyCYdhIj"},"source":["Самый простой вариант дерева решений - так называемый *решающий пень* (decision stump). Отличие решающего пня от решающего дерева заключается в том, что пень всегда состоит только из одной вершины, то есть для его реализации нам не придется заморачиваться с рекурсивным перебором"]},{"cell_type":"markdown","metadata":{"id":"fplJYyweelf7"},"source":["Начнем с того, что научимся считать прирост энтропии после разбиения множества на две части."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hjYSeiAHgzKi","executionInfo":{"status":"ok","timestamp":1682197353911,"user_tz":-180,"elapsed":355,"user":{"displayName":"Kristina Bylkova","userId":"16416973404630871143"}}},"outputs":[],"source":["from math import log2\n","\n","\n","\"\"\"\n","Пошаговый разбор:\n","\n","Шаг 1:\n","y: pd.Series -> изначальная выборка с целевыми переменными\n","0     2\n","1     2\n","2     1\n","3     1\n","4     2\n","5     3\n","Name: target, dtype: int64\n","\n","Шаг 2:\n","probas = y.value_counts(normalize=True) -> Отображение класса на долю вхождений объектов этого класса в выборку\n","1     2 / 6 ~ 0.33333334\n","2     3 / 6 = 0.5\n","3     1 / 6 ~ 0.16666666\n","Name: target, dtype: int64\n","\n","Шаг 3:\n","probas.apply(lambda p: p * log2(p)) -> Умножение вероятности каждого класса на логарифм этой вероятности по определению энтропии по Шеннону\n","\n","Шаг 4:\n","-probas.apply(lambda p: p * log2(p)).sum() -> Суммируем со знаком минус\n","\"\"\"\n","def entropy(targets: pd.Series) -> float:\n","    class_probas = targets.value_counts(normalize=True)\n","    return -class_probas.apply(lambda p: p * log2(p)).sum()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cVewXB4sZM1r","executionInfo":{"status":"ok","timestamp":1682197357669,"user_tz":-180,"elapsed":241,"user":{"displayName":"Kristina Bylkova","userId":"16416973404630871143"}}},"outputs":[],"source":["eps = 1e-7\n","\n","assert abs(entropy(pd.Series(['a', 'a', 'a'])) - 0.) < eps\n","assert abs(entropy(pd.Series(['a', 'a', 'b'])) - 0.9182958340) < eps\n","assert abs(entropy(pd.Series(['a', 'a', 'b', 'b', 'c', 'a', 'a', 'b'])) - 1.4056390622) < eps\n","assert abs(entropy(pd.Series(['a', 'b', 'c', 'd'])) - 2) < eps"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hVDHvQPYboGj","executionInfo":{"status":"ok","timestamp":1682197359447,"user_tz":-180,"elapsed":274,"user":{"displayName":"Kristina Bylkova","userId":"16416973404630871143"}}},"outputs":[],"source":["\"\"\"\n","Важно: прирост информации мы считаем по целевой переменной\n","\"\"\"\n","def information_gain(before_split: pd.Series, split_left: pd.Series, split_right: pd.Series) -> float:\n","    e_before_split = entropy(before_split)\n","    e_left, left_proportion = entropy(split_left), len(split_left) / len(before_split)\n","    e_right, right_proportion = entropy(split_right), len(split_right) / len(before_split)\n","    return e_before_split - (left_proportion * e_left) - (right_proportion * e_right) "]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9GdEfpfHb6X5","executionInfo":{"status":"ok","timestamp":1682197361150,"user_tz":-180,"elapsed":297,"user":{"displayName":"Kristina Bylkova","userId":"16416973404630871143"}}},"outputs":[],"source":["assert abs(information_gain(pd.Series(['a', 'a', 'b', 'b']), pd.Series(['a', 'a']), pd.Series(['b', 'b'])) - 1.0) < eps\n","assert abs(information_gain(pd.Series(['a', 'b', 'c', 'b']), pd.Series(['a', 'c']), pd.Series(['b', 'b'])) - 1.0) < eps\n","assert abs(information_gain(pd.Series(['a', 'b', 'c', 'd']), pd.Series(['a', 'b']), pd.Series(['c', 'd'])) - 1.0) < eps\n","assert abs(information_gain(pd.Series(['a', 'a', 'c', 'd']), pd.Series(['a', 'c']), pd.Series(['a', 'd'])) - 0.5) < eps"]},{"cell_type":"markdown","metadata":{"id":"HlRfYI45gpJx"},"source":["Теперь напишем перебор разделяющих границ внутри одного признака. В нашем случае датасет имеет только числовые признаки, поэтому нам даже не нужно делать никакую предобработку признаков! В случае, если бы данные содержали категориальные признаки, нам нужно было бы сначала преобразовать их при помощи One-Hot кодирования."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XeKlx5Sb5L0"},"outputs":[],"source":["from typing import Tuple\n","\n","\n","\"\"\"\n","Реализуем перебор самым простым способом: переберём фиксированным шагом значения от максимума до минимума\n","\n","Очевидно, что это не всегда выгодно. Например, если у нас есть выбросы, их лучше выкинуть\n","\"\"\"\n","def split_by_feature(x: pd.DataFrame, y: pd.Series, feature: str, steps: int = 100) -> Tuple[float, float]:\n","    min_value, max_value = x[feature].min(), x[feature].max()\n","    thresholds = np.linspace(min_value, max_value, steps)\n","    best_gain, best_threshold = 0, None\n","    for threshold in thresholds:\n","       ### Ваш код\n","    return best_gain, best_threshold"]},{"cell_type":"markdown","metadata":{"id":"HIQFS6KBi0-A"},"source":["Теперь напишем перебор разбиений по всем признакам с выбором лучшего разбиения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLzB1haQi0j4"},"outputs":[],"source":["def split_by_features(x: pd.DataFrame, y: pd.Series, steps: int = 100) -> Tuple[float, str]:\n","    best_gain, best_threshold, best_feature, best_idx = 0, None, None, None\n","    for idx, feature in enumerate(x.columns):\n","        ### Ваш код\n","    return best_gain, best_feature, best_idx, best_threshold"]},{"cell_type":"markdown","metadata":{"id":"jdkHvu3gpYUj"},"source":["Наконец можем описать решающий пень!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMno_hoIpaw8"},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Any, Optional\n","\n","\n","\"\"\"\n","Опишем вершину принятия решений\n","\n","В вершине мы либо храним правило, либо доминантный класс (если это лист)\n","\"\"\"\n","@dataclass\n","class DecisionNode:\n","    feature: Optional[str] = None\n","    feature_idx: Optional[int] = None\n","    threshold: Optional[float] = None\n","    dominative_class: Any = None\n","\n","    @classmethod\n","    def make_leaf(cls, values: pd.Series) -> 'DecisionNode':\n","        class_counts = values.value_counts().to_dict()\n","        dominative_class = max(class_counts, key=lambda c: class_counts[c])\n","        return cls(dominative_class=dominative_class)\n","\n","    @classmethod\n","    def make_node(\n","        cls, best_feature: str, best_feature_idx: int, best_threshold: float\n","    ) -> 'DecisionNode':\n","        return cls(\n","            feature=best_feature,\n","            feature_idx=best_feature_idx,\n","            threshold=best_threshold\n","        )\n","\n","    @property\n","    def is_leaf(self) -> bool:\n","        return bool(self.dominative_class)\n","\n","\n","class DecisionStump:\n","    def __init__(self):\n","        # Будем хранить внутри пня информацию о лучшем признаке и лучшей границе этого признака\n","        self._node: Optional[DecisionNode] = None\n","        self._left_node: Optional[DecisionNode] = None\n","        self._right_node: Optional[DecisionNode] = None\n","\n","    def fit(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n","        _, feature, feature_idx, threshold = split_by_features(X_train, y_train)\n","        x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n","        y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n","        ### Ваш код\n","\n","    def predict(self, X: pd.DataFrame) -> pd.Series:\n","        x = X.copy()\n","        node = self._node\n","        left_samples = x[x[node.feature] < node.threshold]\n","        right_samples = x[x[node.feature] >= node.threshold]\n","        ### Ваш код\n","        return x.pred\n","\n","    def fit_predict(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:\n","        self.fit(X, y)\n","        return self.predict(X)"]},{"cell_type":"markdown","metadata":{"id":"xJahOXvQ8oYV"},"source":["Проблема: оказывается (кто бы мог подумать!) решающий пень хорошо подходит только для бинарной классификации. Поэтому возьмем из всех примеров только два класса и построим для них пень."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvRH6-fm8ZTX"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","reduced_df = df[df.target.apply(lambda t: t in ['setosa', 'versicolor'])]\n","X, y = reduced_df.drop(columns=['target']), reduced_df.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.35, random_state=42)\n","\n","print(len(X_train), len(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Shff4Wf6q1O"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","stump = DecisionStump()\n","stump.fit(X_train, y_train)\n","y_pred = stump.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V6T1vGg9LgE"},"outputs":[],"source":["stump._node.feature, stump._node.threshold"]},{"cell_type":"markdown","metadata":{"id":"kb-S-1zA7gp7"},"source":["Однако решающий пень всё ещё можно приспособить для многоклассовой классификации. Например, его можно обернуть в какую-нибудь хитрую функцию, которая бы училась предсказывать примеры из узла, если в нём не достигнуто какое-нибудь заранее определённое преимущество одного из классов..."]},{"cell_type":"markdown","metadata":{"id":"LssXLnIqcASs"},"source":["## **Сделай сам**: решающее дерево"]},{"cell_type":"markdown","metadata":{"id":"6Io2oMqN8NAd"},"source":["...И тут мы понимаем, что это не что иное как дерево решений.\n","\n","По сути, основные функции для реализации дерева решений у нас готовы. Нам нужно только чуть-чуть переписать класс `DecisionNode` и по-человечески написать рекурсивный перебор признаков с жадным выбором."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xvr8U9ZU-ilz"},"outputs":[],"source":["@dataclass\n","class DecisionNode:\n","    # Решающие признаки остаются как есть\n","    feature: Optional[str] = None\n","    feature_idx: Optional[int] = None\n","    threshold: Optional[float] = None\n","    # Добавляются ссылки на левую и правую вершины\n","    left: Optional['DecisionNode'] = None\n","    right: Optional['DecisionNode'] = None\n","    # Класс в листе остается как есть\n","    dominative_class: Any = None\n","\n","    @property\n","    def is_leaf(self) -> bool:\n","        return bool(self.dominative_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPdF8apfcCbS"},"outputs":[],"source":["class DecisionTree:\n","    def __init__(self, max_impurity_spllit: float = 0.01, min_leaf_split=2, max_depth=None):\n","        self._root: Optional[DecisionNode] = None\n","        # В качестве простых эвристики для остановки возьмем:\n","        # - максимальную глубину ветви\n","        # - минимально допустимое число примеров в узле\n","        # - долю \"примесей\" в узле\n","        self._max_impurity_spllit = max_impurity_spllit\n","        self._min_leaf_split = min_leaf_split\n","        self._max_depth = max_depth\n","\n","    def _fit(self, X_train: pd.DataFrame, y_train: pd.Series, depth=0):\n","        # проверим, можно ли вернуть лист\n","        classes_probas = y_train.value_counts(normalize=True).to_dict()\n","        dominative_class = max(classes_probas, key=lambda c: classes_probas[c])\n","        impurity = 1 - classes_probas[dominative_class]\n","        leaf_size = len(X_train)\n","        if impurity <= self._max_impurity_spllit or leaf_size < self._min_leaf_split or (self._max_depth and depth >= self._max_depth):\n","            return DecisionNode(dominative_class=dominative_class)\n","        # если нет, продолжаем идти рекурсивно\n","        # делаем перебор максимум в 10 точках\n","        _, feature, feature_idx, threshold = split_by_features(X_train, y_train, 10)\n","        if not feature:  # если не получается выбрать оптимальный признак для разбиения - завершаем перебор\n","            return DecisionNode(dominative_class=dominative_class)\n","        x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n","        y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n","        left_node = self._fit(x_left, y_left, depth + 1)\n","        right_node = self._fit(x_right, y_right, depth + 1)\n","        return DecisionNode(\n","            feature=feature,\n","            feature_idx=feature_idx,\n","            threshold=threshold,\n","            left=left_node,\n","            right=right_node,\n","        )\n","\n","    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n","        self._root = self._fit(X_train, y_train)\n","\n","    def _predict(self, sample: pd.Series, node: DecisionNode) -> Any:\n","        if node.is_leaf:\n","            return node.dominative_class\n","        if sample[node.feature] < node.threshold:\n","            return self._predict(sample, node.left)\n","        return self._predict(sample, node.right)\n","\n","    def predict(self, X: pd.DataFrame) -> pd.Series:\n","        return X.apply(lambda sample: self._predict(sample, self._root), axis=1)\n","\n","    def fit_predict(self, X_train: pd.DataFrame, y_train: pd.Series):\n","        self.fit(X_train, y_train)\n","        return self.predict(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDWJbXczwoEg"},"outputs":[],"source":["X, y = df.drop(columns=['target']), df.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)\n","\n","print(len(X_train), len(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bF0WxfxWFaAd"},"outputs":[],"source":["tree = DecisionTree()\n","tree.fit(X_train, y_train)\n","y_pred = tree.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"dEYI59j2cOFu"},"source":["## Возможности sklearn"]},{"cell_type":"markdown","metadata":{"id":"VnAxUlxrBIf5"},"source":["### Сравнение на нормальном датасете"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nz6xwpFZNyhp"},"outputs":[],"source":["# !pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRg1iRJMBMAJ"},"outputs":[],"source":["!gdown 1oCrlxirolbpiBmYtLv2eLjI30k4swFgD -O pulsar.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EE01aF7RGOMW"},"outputs":[],"source":["df = pd.read_csv('pulsar.csv')\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icDjkzSMkx1J"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MioNPCH5KAEU"},"outputs":[],"source":["df.target_class.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoHZoZFfJH5F"},"outputs":[],"source":["na_columns = df.isna().any()[df.isna().any()]  # выведем только колонки с NaN'ами\n","na_columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXmXgSdqm-QS"},"outputs":[],"source":["# заполним пропуски в каждой колонке средним по непустым значениям\n","for na_column in na_columns.index:\n","    df.loc[df[na_column].isna(), na_column] = df[~df[na_column].isna()][na_column].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3CQbbzvpGBR"},"outputs":[],"source":["df.isna().any().any()  # колонок с пропусками не осталось"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iim1GLl1Gae-"},"outputs":[],"source":["X, y = df.drop(columns=['target_class']), df.target_class.apply(int).apply(str)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.65, random_state=42)\n","\n","len(X_train), len(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gD1EsEeHGVDG"},"outputs":[],"source":["tree = DecisionTree()\n","tree.fit(X_train, y_train)\n","y_pred = tree.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"hhEWDB8xsTWu"},"source":["Неплохо, но довольно долго, не так ли?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbN8PgrkIuX1"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","\n","sklearn_tree = DecisionTreeClassifier()\n","sklearn_tree.fit(X_train, y_train)\n","y_pred = sklearn_tree.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"Py2F45uYsW_W"},"source":["Sklearn справился **значительно** быстрее. Да, мы могли поставить разные отсечения (например, задать `max_depth=5`), но sklearn из коробки дает почти такое же качество при значительно большей скорости."]},{"cell_type":"markdown","metadata":{"id":"aQBlE30rcSvd"},"source":["### Гиперпараметры"]},{"cell_type":"markdown","metadata":{"id":"YUOq14JgtInF"},"source":["Однако основная проблема в том, что мы сильно переобучились. Давайте посмотрим на то, как меняется качество модели на обучающей и отложенной выборках при изменении некоторых гиперпараметров.\n","\n","Дисклеймер: на самом деле так перебирать гиперпараметры **нельзя**! Если вы в рамках реальной задачи хотите подобрать оптимальные гиперпараметры, вам нужно разбить данные на *три* части:\n","\n","1. `train` - на этих данных вы обучаете модели, перебирая значения гиперпараметров;\n","2. `valid` - на этих данных вы валидируете обученные модели и выбираете $k$ лучших (часто $k$ берут небольшим, от 3 до 5);\n","3. `test` - на этих данных вы проверяете $k$ лучших кандидатов и выбираете тот набор гиперпараметров, который показал лучший результат на тестовом множестве."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8ELtiqUcSSs"},"outputs":[],"source":["import seaborn as sns\n","\n","\n","def plot_fitting_curve(parameter: str, values: list, score, X_train, X_test, y_train, y_test):\n","    train_curve = []\n","    test_curve = []\n","    for value in values:\n","        # инициализация объекта из словаря\n","        # распаковка словаря через ** эквивалентна перечислению аргументов\n","        model = DecisionTreeClassifier(**{parameter: value})\n","        model.fit(X_train, y_train)\n","        y_pred_train, y_pred_test = model.predict(X_train), model.predict(X_test)\n","        train_curve.append(score(y_train, y_pred_train, average='macro'))\n","        test_curve.append(score(y_test, y_pred_test, average='macro'))\n","    sns.lineplot(x=values, y=train_curve)\n","    sns.lineplot(x=values, y=test_curve)"]},{"cell_type":"markdown","metadata":{"id":"xRVFLgzOw8-b"},"source":["Попробуем перебрать максимальную глубину дерева"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TnSRkP9uz3M"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","\n","plot_fitting_curve('max_depth', np.arange(1, 20), f1_score, X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"tkwxph8KxK0z"},"source":["Видно, что на глубине больше трех наступает переобучение и качество на тестовой выборке начинает падать.\n","\n","Далее попробуем перебрать минимальное число примеров в узле для разбиения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFj2Cr_Iwqeb"},"outputs":[],"source":["plot_fitting_curve('min_samples_split', np.arange(5, 100, 2), f1_score, X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"stGj-goTxeOz"},"source":["В случае с этим параметром видно, что его увеличение хорошо влияет на сходимость модели. В какой-то момент увеличение этого параметра перестает влиять на значение f1-меры на обучении и тесте.\n","\n","Дальше попробуем перебрать параметр `max_features`, отвечающий за максимальное число признаков, используемых для разбиения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxrWzPovx9V8"},"outputs":[],"source":["plot_fitting_curve('max_features', np.linspace(0.1, 1, 20), f1_score, X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"vieEHfJscVaU"},"source":["### Решающая граница"]},{"cell_type":"markdown","metadata":{"id":"dxIYF0rozqrx"},"source":["Наконец посмотрим, как выглядит решающая граница, которую строит дерево решений. Для этого возьмем датасет с ирисами и выберем из него два произвольных признака."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gfc5UBWTcX6N"},"outputs":[],"source":["df = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n","df['target'] = iris_dataset.target\n","df['target'] = df.target.apply(lambda v: iris_dataset.target_names[v])\n","\n","print(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkDk3aUu0AM4"},"outputs":[],"source":["df.corr()"]},{"cell_type":"markdown","metadata":{"id":"jpskEjKC0eP4"},"source":["Оставим длину чашелистника и ширину чашелистника"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ldet0g1y0msz"},"outputs":[],"source":["X = df.drop(columns=['target'])[['sepal length (cm)', 'sepal width (cm)']].rename(columns={'sepal length (cm)': 0, 'sepal width (cm)': 1})\n","y = df.target.map({'setosa': 0, 'versicolor': 1, 'virginica': 2})\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZQQke-fz43A"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score\n","from matplotlib.colors import ListedColormap\n","\n","\n","def get_meshgrid(data, step=.05, border=.5,):\n","    x_min, x_max = data.iloc[:, 0].min() - border, data.iloc[:, 0].max() + border\n","    y_min, y_max = data.iloc[:, 1].min() - border, data.iloc[:, 1].max() + border\n","    \n","    return np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n","\n","def plot_decision_surface(estimator, train_data, train_labels, test_data, test_labels):\n","    estimator.fit(train_data, train_labels)\n","    \n","    plt.figure(figsize = (16, 6))\n","    light_colors = ListedColormap(['lightyellow','lightcoral','lightgreen'])\n","    colors = ListedColormap(['yellow', 'red', 'green'])\n","    \n","    #plot decision surface on the train data \n","    plt.subplot(1,2,1)\n","    xx, yy = get_meshgrid(train_data)\n","    mesh_predictions = np.array(estimator.predict(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n","    plt.pcolormesh(xx, yy, mesh_predictions, cmap=light_colors)\n","    plt.scatter(train_data.iloc[:, 0], train_data.iloc[:, 1], c=train_labels, s=40, cmap=colors)\n","    plt.title('Train data, accuracy={}'.format(accuracy_score(train_labels, estimator.predict(train_data))))\n","    \n","    #plot decision surface on the test data\n","    plt.subplot(1,2,2)\n","    plt.pcolormesh(xx, yy, mesh_predictions, cmap=light_colors)\n","    plt.scatter(test_data.iloc[:, 0], test_data.iloc[:, 1], c=test_labels, s=40, cmap=colors)\n","    plt.title('Test data, accuracy={}'.format(accuracy_score(test_labels, estimator.predict(test_data))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChdSBwtn05tn"},"outputs":[],"source":["sklearn_tree = DecisionTreeClassifier(max_depth=2)\n","\n","plot_decision_surface(sklearn_tree, X_train, y_train, X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mIyMEI51Mzz"},"outputs":[],"source":["sklearn_tree = DecisionTreeClassifier(max_depth=100)\n","\n","plot_decision_surface(sklearn_tree, X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"JvV2bGsn4ZOF"},"source":["Если сравнить эти четыре картинки, то можно заметить, как возрастает сложность разделяющей границы в зависимости от глубины дерева. При этом обратите внимание, что все разделяющие границы проходят по прямой относительно оси. Это связано с тем, что разделяющее правило $X < x_0$ соответствует прямой, проходящей через $x_0$ перпендикулярно оси $X$."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":0}